{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNuP8m0WC1hH"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Deanis/MLEngineering_Capstone_Group3/blob/main/notebooks/pipeline.i:pynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAA_0UJpC__P"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdw-CtYLCxAd",
        "outputId": "34c69377-51d3-4c7c-f636-f3e6d7b46d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kfp in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.28.1)\n",
            "Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (8.1.6)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.15)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.11.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.17.3)\n",
            "Requirement already satisfied: kfp-pipeline-spec==0.2.2 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.2.2)\n",
            "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.0.0)\n",
            "Requirement already satisfied: kubernetes<27,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (26.1.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.13.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (3.20.3)\n",
            "Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.10.1)\n",
            "Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.9.0)\n",
            "Requirement already satisfied: urllib3<2.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (1.26.16)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.3)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.10.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.10.2)\n",
            "Requirement already satisfied: shapely<2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.59.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.56.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2023.5.7)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (67.7.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (1.6.1)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install kfp pandas transformers google-cloud-storage google-cloud-aiplatform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9mPNMgt9C1ZD"
      },
      "outputs": [],
      "source": [
        "import kfp\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import component\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from google.cloud import aiplatform\n",
        "import os\n",
        "from typing import NamedTuple\n",
        "\n",
        "\n",
        "# Constants\n",
        "bucket_name = 'blank-to-bard'\n",
        "file_name = 'english-dataset.csv'\n",
        "project_id = 'ml-class-group-3-capstone'\n",
        "model_name = 'blank-to-bard'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZUqtZqNDNWE"
      },
      "source": [
        "## Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3yG6IIl4DM7N"
      },
      "outputs": [],
      "source": [
        "@dsl.component(packages_to_install=['pandas', 'numpy', 'google-cloud-storage', 'transformers', 'scikit-learn', 'tensorflow'])\n",
        "def train_model(bucket_name: str, file_name: str):\n",
        "    from google.cloud import storage\n",
        "    import pandas as pd\n",
        "    from io import BytesIO\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from io import BytesIO\n",
        "    from transformers import DistilBertTokenizer\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import tensorflow as tf\n",
        "    from collections import namedtuple\n",
        "\n",
        "    # Initialize a client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Access the bucket and the file\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    # Download the data into a pandas dataframe\n",
        "    data = blob.download_as_text()\n",
        "    data = pd.read_csv(BytesIO(bytes(data, 'utf-8')))\n",
        "\n",
        "    from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Tokenizer\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    # Model\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    # Tokenize the data\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in data['text']:\n",
        "        inputs = tokenizer.encode_plus(text, add_special_tokens=True, max_length=128, pad_to_max_length=True,\n",
        "                                      return_attention_mask=True, return_tensors='tf')\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        attention_masks.append(inputs['attention_mask'])\n",
        "\n",
        "    input_ids = tf.concat(input_ids, axis=0)\n",
        "    attention_masks = tf.concat(attention_masks, axis=0)\n",
        "\n",
        "    # Ensure labels are in the same order and format as the inputs\n",
        "    labels = tf.convert_to_tensor(data['label'])\n",
        "\n",
        "    # Convert TensorFlow tensors to numpy arrays before splitting\n",
        "    input_ids = input_ids.numpy()\n",
        "    attention_masks = attention_masks.numpy()\n",
        "    labels = labels.numpy()\n",
        "\n",
        "    # Split the data\n",
        "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2023, test_size=0.2)\n",
        "\n",
        "    # Convert numpy arrays back to tensors\n",
        "    train_inputs = tf.convert_to_tensor(train_inputs)\n",
        "    validation_inputs = tf.convert_to_tensor(validation_inputs)\n",
        "    train_labels = tf.convert_to_tensor(train_labels)\n",
        "    validation_labels = tf.convert_to_tensor(validation_labels)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit([train_inputs, attention_masks[:len(train_inputs)]], train_labels, batch_size=32, epochs=4, validation_split=0.1)\n",
        "\n",
        "\n",
        "    # Define model directory\n",
        "    model_dir = './pipeline-model'\n",
        "\n",
        "    model.save_pretrained(model_dir, saved_model=True)\n",
        "\n",
        "    import os\n",
        "\n",
        "    # Define GCS bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Walk through all directories and sub-directories\n",
        "    for root, dirs, files in os.walk(model_dir):\n",
        "        for file in files:\n",
        "            local_file = os.path.join(root, file)\n",
        "            remote_file = local_file[len(model_dir):]\n",
        "\n",
        "            # Ensure the path doesn't start with a slash\n",
        "            if remote_file.startswith('/'):\n",
        "                remote_file = remote_file[1:]\n",
        "\n",
        "            blob = bucket.blob(f'pipeline-model/{remote_file}')\n",
        "            blob.upload_from_filename(local_file)\n",
        "\n",
        "\n",
        "\n",
        "@dsl.component(packages_to_install=['google-cloud-aiplatform'])\n",
        "def upload_model_to_vertex_ai_op(\n",
        "    project_id: str,\n",
        "    bucket_name: str,\n",
        "    model_name: str,\n",
        "):\n",
        "    from google.cloud import aiplatform\n",
        "\n",
        "    aiplatform.init(project=project_id)\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        artifact_uri=\"gs://\" + bucket_name + \"/pipeline-model/saved_model/1/\",\n",
        "        serving_container_image_uri='gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest',\n",
        "    )\n",
        "\n",
        "    model.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHc_ODqSM1GT"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WTpZfdrFM1S5"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    name='Training pipeline',\n",
        "    description='A pipeline that downloads, pre-processes data, trains a model, saves it and deploys it.'\n",
        ")\n",
        "def training_pipeline(bucket_name: str, file_name: str, project_id: str, model_name: str):\n",
        "    train_op = train_model(bucket_name=bucket_name, file_name=file_name)\n",
        "    upload_model_op = upload_model_to_vertex_ai_op(project_id=project_id, bucket_name=bucket_name, model_name=model_name)\n",
        "\n",
        "    upload_model_op.after(train_op)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzf0d2U_PMxl"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Kw-YngmUPOGl"
      },
      "outputs": [],
      "source": [
        "# Compile the pipeline\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=training_pipeline,\n",
        "    package_path='training_pipeline.json'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "STVjsRlGLhEL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
