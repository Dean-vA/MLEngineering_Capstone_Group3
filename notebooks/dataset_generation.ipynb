{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6U7ovXTeuSS"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Deanis/MLEngineering_Capstone_Group3/blob/main/notebooks/dataset_generation.ipyn\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAdeN8n7do9Z"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "In this notebook we generate a dataset using Open AI API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nj3EX-etynS"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uEgAfwRfMDf"
      },
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtkSWvBHhG3G",
        "outputId": "1e4027d0-0c37-4495-aedc-129fd63dfaec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.234-py3-none-any.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.18)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.0.6,>=0.0.5 (from langchain)\n",
            "  Downloading langsmith-0.0.5-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.9 langchain-0.0.234 langsmith-0.0.5 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AoVG_EpAd0KU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BabWUcQXiI7U"
      },
      "source": [
        "**Chain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldbda75iiGTq",
        "outputId": "a7be2438-3311-4d02-a785-51092e265cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"Je suis allé au théâtre hier soir.\"=>1\n",
            "\"Je hier soir suis allé au théâtre.\"=>0\n",
            "\"Elle a un chat noir.\"=>1\n",
            "\"Elle un chat noir a.\"=>0\n",
            "\"Nous aimons manger des fruits le matin.\"=>1\n",
            "\"Des fruits le matin nous aimons manger.\"=>0\n",
            "\"Le livre est sur la table.\"=>1\n",
            "\"Sur la table est le livre.\"=>0\n",
            "\"Paul a trois chiens et une petite chatte.\"=>1\n",
            "\"Paul a trois et une petite chatte chiens.\"=>0\n",
            "\"J'aime le café chaud le matin.\"=>1\n",
            "\"Le café chaud le matin j'aime.\"=>0\n",
            "\"Mon père travaille dans une entreprise internationale.\"=>1\n",
            "\"Dans une entreprise internationale mon père travaille.\"=>0\n",
            "\"C'est une belle journée pour se promener dans le parc.\"=>1\n",
            "\"Une belle journée pour se promener dans le parc c'est.\"=>0\n",
            "\"Martin et Sophie sont allés au cinéma hier.\"=>1\n",
            "\"Au cinéma hier sont allés Martin et Sophie.\"=>0\n",
            "\"J'ai faim, je vais préparer le dîner.\"=>1\n",
            "\"Je vais préparer le dîner, j'ai faim.\"=>0\n",
            "\"Les enfants aiment jouer dans le jardin après l'école.\"=>1\n",
            "\"Dans le jardin après l'école les enfants aiment jouer.\"=>0\n",
            "\"Je dois étudier pour mon examen demain.\"=>1\n",
            "\"Pour mon examen demain je dois étudier.\"=>0\n"
          ]
        }
      ],
      "source": [
        "from langchain import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "\n",
        "llm = OpenAI(temperature=0.9, model_name=\"gpt-4\")\n",
        "\n",
        "template = \"\"\"\n",
        "Your assignment is to create a dataset which will be utilized to train a machine learning classifier.\n",
        "\n",
        "The 'blank-to-bard' model is intended to facilitate the learning process of writing idiomatic {language}.\n",
        "\n",
        "Your dataset should include two types of sentences:\n",
        "\n",
        "Well-written sentences, labeled as '1'.\n",
        "Sentences with common errors often made by non-native speakers or children. Specifically, incorporate sentences that are grammatically correct, but stylistically poor. These should be labeled as '0'.\n",
        "\n",
        "Give 100 examples.\n",
        "\n",
        "The required format for your response the follwing (sentences should be in {language}):\n",
        "\n",
        "To be or not to be that is the question.=>1\n",
        "I go shop for buying food many.=>0\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"language\"])\n",
        "\n",
        "chain = LLMChain(prompt=prompt,llm=llm)\n",
        "\n",
        "completion = chain.predict(language=\"french\")\n",
        "\n",
        "print(completion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zodlIcoZrEhg"
      },
      "source": [
        "Creating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3VvvgDmWiMEI"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "LANGUAGE = \"english\"\n",
        "\n",
        "# Prepare data for csv\n",
        "data = []\n",
        "\n",
        "for _ in range(100):\n",
        "  completion = chain.predict(language=LANGUAGE)\n",
        "\n",
        "  # Split into lines\n",
        "  lines = completion.strip().split(\"\\n\")\n",
        "\n",
        "  for line in lines:\n",
        "      # Extract text and label using regex\n",
        "      for line in lines:\n",
        "          text, label = line.split(\"=>\")\n",
        "          text = text.strip('\"')\n",
        "          data.append([text, label])\n",
        "\n",
        "# Define column names\n",
        "column_names = [\"text\", \"label\"]\n",
        "\n",
        "# Write to csv\n",
        "with open(LANGUAGE + '-dataset.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(column_names)\n",
        "    writer.writerows(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3V0uXuNrdjD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
