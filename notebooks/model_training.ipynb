{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r5Sa2QlFMwmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Deanis/MLEngineering_Capstone_Group3/blob/main/notebooks/model_training.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "vPrLj5s9L-ZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "aj8VTrIbMBqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bajaBTuuMnZH",
        "outputId": "a316b2bb-d0ed-4733-b9ff-1510aa157a46"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-fa03e7445759>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas transformers google-cloud-storage google-cloud-aiplatform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gz2f55dVL8-f",
        "outputId": "e12de097-0388-40b5-e7c3-11736e12956a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m5.5/7.2 MB\u001b[0m \u001b[31m166.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m171.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.28.0-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.17.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.10.0)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
            "  Downloading google_cloud_resource_manager-1.10.2-py2.py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shapely<2.0.0 (from google-cloud-aiplatform)\n",
            "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.59.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.56.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.5.0)\n",
            "Installing collected packages: tokenizers, safetensors, shapely, huggingface-hub, transformers, google-cloud-resource-manager, google-cloud-aiplatform\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.0.1\n",
            "    Uninstalling shapely-2.0.1:\n",
            "      Successfully uninstalled shapely-2.0.1\n",
            "Successfully installed google-cloud-aiplatform-1.28.0 google-cloud-resource-manager-1.10.2 huggingface-hub-0.16.4 safetensors-0.3.1 shapely-1.8.5.post1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "qZd3_yN9MFV8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide your bucket name and file name\n",
        "bucket_name = 'blank-to-bard'\n",
        "file_name = 'english-dataset.csv'\n",
        "\n",
        "# Define your project ID and model name\n",
        "project_id = 'ml-class-group-3-capstone'\n",
        "model_name = 'blank-to-bard'\n"
      ],
      "metadata": {
        "id": "ehk-HQOHZyss"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch the dataset from GCS:"
      ],
      "metadata": {
        "id": "9Vp0fUfhNzWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "\n",
        "# Initialize a client\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Access the bucket and the file\n",
        "bucket = storage_client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_name)\n",
        "\n",
        "# Download the data into a pandas dataframe\n",
        "data = blob.download_as_text()\n",
        "data = pd.read_csv(BytesIO(bytes(data, 'utf-8')))\n",
        "\n",
        "# Check the first few records\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_Z8H_W0NmDH",
        "outputId": "4bf305f1-3ac9-4005-d453-d2860c18cb68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           text  label\n",
            "0  The quick brown fox jumps over the lazy dog.      1\n",
            "1  Dog lazy the over jumps fox brown quick the.      0\n",
            "2                       The cat sat on the mat.      1\n",
            "3                       Cat the on mat the sat.      0\n",
            "4                  She walks to work every day.      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data, train, and evaluate a model using DistilBERT:"
      ],
      "metadata": {
        "id": "Nso961qkZaLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Model\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize the data\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in data['text']:\n",
        "    inputs = tokenizer.encode_plus(text, add_special_tokens=True, max_length=128, pad_to_max_length=True,\n",
        "                                   return_attention_mask=True, return_tensors='tf')\n",
        "    input_ids.append(inputs['input_ids'])\n",
        "    attention_masks.append(inputs['attention_mask'])\n",
        "\n",
        "input_ids = tf.concat(input_ids, axis=0)\n",
        "attention_masks = tf.concat(attention_masks, axis=0)\n",
        "\n",
        "# Ensure labels are in the same order and format as the inputs\n",
        "labels = tf.convert_to_tensor(data['label'])\n",
        "\n",
        "# Convert TensorFlow tensors to numpy arrays before splitting\n",
        "input_ids = input_ids.numpy()\n",
        "attention_masks = attention_masks.numpy()\n",
        "labels = labels.numpy()\n",
        "\n",
        "# Split the data\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2023, test_size=0.2)\n",
        "\n",
        "# Convert numpy arrays back to tensors\n",
        "train_inputs = tf.convert_to_tensor(train_inputs)\n",
        "validation_inputs = tf.convert_to_tensor(validation_inputs)\n",
        "train_labels = tf.convert_to_tensor(train_labels)\n",
        "validation_labels = tf.convert_to_tensor(validation_labels)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit([train_inputs, attention_masks[:len(train_inputs)]], train_labels, batch_size=32, epochs=4, validation_split=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMfrTbgKZLu4",
        "outputId": "d29384e0-e2ec-45ce-dec2-095c57279bfe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "1079/1079 [==============================] - 98s 65ms/step - loss: 0.0564 - accuracy: 0.9799 - val_loss: 0.0114 - val_accuracy: 0.9977\n",
            "Epoch 2/4\n",
            "1079/1079 [==============================] - 56s 52ms/step - loss: 0.0129 - accuracy: 0.9967 - val_loss: 0.0180 - val_accuracy: 0.9963\n",
            "Epoch 3/4\n",
            "1079/1079 [==============================] - 56s 52ms/step - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.0051 - val_accuracy: 0.9982\n",
            "Epoch 4/4\n",
            "1079/1079 [==============================] - 56s 52ms/step - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0038 - val_accuracy: 0.9984\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bbb7ef33ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the trained model and upload it to GCS:\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "cpuAWRz9Z2Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define GCS bucket\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "# Define model directory\n",
        "model_dir = './model'\n",
        "\n",
        "# Walk through all directories and sub-directories\n",
        "for root, dirs, files in os.walk(model_dir):\n",
        "    for file in files:\n",
        "        local_file = os.path.join(root, file)\n",
        "        remote_file = local_file[len(model_dir):]\n",
        "\n",
        "        # Ensure the path doesn't start with a slash\n",
        "        if remote_file.startswith('/'):\n",
        "            remote_file = remote_file[1:]\n",
        "\n",
        "        blob = bucket.blob(f'model/{remote_file}')\n",
        "        blob.upload_from_filename(local_file)\n"
      ],
      "metadata": {
        "id": "xEFXsjkOZ2wX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy the model on Vertex AI:"
      ],
      "metadata": {
        "id": "9Uba2gmgbHPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=project_id)\n",
        "\n",
        "# gs://blank-to-bard/model/saved_model/1/saved_model.pb\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=model_name,\n",
        "    artifact_uri=\"gs://\" + bucket_name + \"/model/saved_model/1/\",\n",
        "    serving_container_image_uri='gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest',\n",
        ")\n",
        "\n",
        "model.wait()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utvf-maVbG7X",
        "outputId": "80e8b8bc-273c-4954-db7f-09ff7f502003"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Creating Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create Model backing LRO: projects/105876446030/locations/us-central1/models/2824383687989133312/operations/1081997713215586304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/105876446030/locations/us-central1/models/2824383687989133312/operations/1081997713215586304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created. Resource name: projects/105876446030/locations/us-central1/models/2824383687989133312@1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/105876446030/locations/us-central1/models/2824383687989133312@1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To use this Model in another session:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model = aiplatform.Model('projects/105876446030/locations/us-central1/models/2824383687989133312@1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/105876446030/locations/us-central1/models/2824383687989133312@1')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an endpoint to deploy your model:\n",
        "\n"
      ],
      "metadata": {
        "id": "UH0XehkUbSun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = aiplatform.Endpoint.create(\n",
        "    display_name=\"blank-to-bard\",\n",
        ")\n",
        "\n",
        "endpoint.deploy(\n",
        "    model=model,\n",
        "    deployed_model_display_name=model_name,\n",
        "    traffic_percentage=100,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhE7zr0bbSY6",
        "outputId": "37c5f909-954a-41e3-ca5c-dc60be7ddc97"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Endpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Creating Endpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create Endpoint backing LRO: projects/105876446030/locations/us-central1/endpoints/8571964173955629056/operations/1804825453408550912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/105876446030/locations/us-central1/endpoints/8571964173955629056/operations/1804825453408550912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoint created. Resource name: projects/105876446030/locations/us-central1/endpoints/8571964173955629056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/105876446030/locations/us-central1/endpoints/8571964173955629056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To use this Endpoint in another session:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "endpoint = aiplatform.Endpoint('projects/105876446030/locations/us-central1/endpoints/8571964173955629056')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/105876446030/locations/us-central1/endpoints/8571964173955629056')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deploying Model projects/105876446030/locations/us-central1/models/2824383687989133312 to Endpoint : projects/105876446030/locations/us-central1/endpoints/8571964173955629056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Deploying Model projects/105876446030/locations/us-central1/models/2824383687989133312 to Endpoint : projects/105876446030/locations/us-central1/endpoints/8571964173955629056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using default machine_type: n1-standard-2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Using default machine_type: n1-standard-2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deploy Endpoint model backing LRO: projects/105876446030/locations/us-central1/endpoints/8571964173955629056/operations/2746077775528984576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/105876446030/locations/us-central1/endpoints/8571964173955629056/operations/2746077775528984576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Endpoint model deployed. Resource name: projects/105876446030/locations/us-central1/endpoints/8571964173955629056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/105876446030/locations/us-central1/endpoints/8571964173955629056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sample request"
      ],
      "metadata": {
        "id": "IAXYm9EtjHGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth print-access-token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoohKmFKsWC_",
        "outputId": "7a8eb69c-bd7d-466b-fd06-6298a742eeec"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ya29.a0AbVbY6ODSjVOmkZoputAIDSil3w_dbIcs9RbDOoZ4Y1h2fveQN5Mny8hqlNaHXZ7AlJ58oGkoumkV-gPy2LqgJ3X5-ZjBATvdAYaooTB9VujkZUc8AXzCTKsZZXMbIXH_hgPnJA8S87XriBvam3Lw-NvJPqGaCgYKAZcSARISFQFWKvPlUEKQ7Iip_fIpfZ2Xm96HSA0163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "access_token=\"ya29.a0AbVbY6ODSjVOmkZoputAIDSil3w_dbIcs9RbDOoZ4Y1h2fveQN5Mny8hqlNaHXZ7AlJ58oGkoumkV-gPy2LqgJ3X5-ZjBATvdAYaooTB9VujkZUc8AXzCTKsZZXMbIXH_hgPnJA8S87XriBvam3Lw-NvJPqGaCgYKAZcSARISFQFWKvPlUEKQ7Iip_fIpfZ2Xm96HSA0163\"\n",
        "\n",
        "endpoint_id = \"55657278598021120\"\n",
        "\n",
        "# text = \"Me no speak\"\n",
        "text = \"To be or not to be that is the question.\"\n",
        "\n",
        "# Tokenize the text\n",
        "inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "# Prepare a dictionary matching the model's expected input format\n",
        "instance = {key: value.numpy().tolist()[0] for key, value in inputs.items()}\n",
        "\n",
        "input_data = {\n",
        "    \"instances\": [instance]\n",
        "}\n",
        "\n",
        "# Convert input data to JSON string\n",
        "input_data_json = json.dumps(input_data)\n",
        "\n",
        "# Define the API endpoint URL\n",
        "api_url = f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{project_id}/locations/us-central1/endpoints/{endpoint_id}:predict\"\n",
        "\n",
        "# Set the request headers\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {access_token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Send the POST request\n",
        "response = requests.post(api_url, headers=headers, data=input_data_json)\n",
        "\n",
        "# Parse the response\n",
        "if response.status_code == 200:\n",
        "    prediction = response.json()\n",
        "    print(\"Prediction:\", prediction)\n",
        "else:\n",
        "    print(\"Prediction request failed with status code:\", response.status_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htND5ZWkjGld",
        "outputId": "f874e072-32a6-420c-9b63-e1c44c862b79"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: {'predictions': [[-4.57415724, 4.82015753]], 'deployedModelId': '7583272323143172096', 'model': 'projects/105876446030/locations/us-central1/models/2824383687989133312', 'modelDisplayName': 'blank-to-bard', 'modelVersionId': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_19h19fFhoum"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}